import asyncio
import contextlib
import glob
import logging
import os
import pathlib
import re
import shlex
import typing
from . import transforms
from . import util


LOGGER = logging.getLogger(__name__)


class ArtifactFactory(type):
    """
    Metaclass that creates :class:`Artifact` instances or returns existing instances.

    Attributes:
        REGISTRY: Mapping of artifact names to :class:`Artifact` instances. The registry is used to
            keep track of all artifacts and ensure artifact names are unique.
    """
    # Declare using an old-school type hint because Sphinx struggles with circular references.
    REGISTRY = {}  # type: dict[str, Artifact]

    def __init__(self, name, bases, members):
        super(ArtifactFactory, self).__init__(name, bases, members)

    def __call__(self, name, *args, **kwargs):
        if isinstance(name, pathlib.Path):
            name = str(name)
        # Evaluate the fully qualified name in light of the current group stack.
        name = Group.evaluate_qualified_name(name)
        # Try to retrieve the instance from the registry.
        if instance := self.REGISTRY.get(name):
            if (cls := instance.__class__) is not self:
                raise ValueError(f"cannot create instance of {self} because artifact `{name}` with "
                                 f"type {cls} already exists")
            return instance
        # Create a new instance, register it with the current group if any, and add it to the
        # registry.
        instance = super(ArtifactFactory, self).__call__(name, *args, **kwargs)
        Group.append(instance)
        self.REGISTRY[name] = instance
        return instance


class Artifact(util.Once, metaclass=ArtifactFactory):
    """
    Artifact generated by a transform.

    Args:
        name: Unique name of the artifact.
        expected_digest: Digest expected when the artifact is available.
        metadata: Metadata which may persist across runs.

    Raises:
        ValueError: If the given :code:`name` is already in use by another artifact.

    Attributes:
        children: Transforms that consume this artifact; empty if the artifact is a leaf of the
            directed acyclic graph.
        parent: Transform that generates this artifact; :code:`None` if the artifact is a root
            of the directed acyclic graph.
        digest: Concise summary of the artifact; :code:`None` if the artifact does not exist, cannot
            be summarized, or should always be generated using its :attr:`parent` transform.
        metadata: Metadata which may persist across runs.
    """
    def __init__(self, name: str, expected_digest: str = None, metadata: dict = None) -> None:
        super().__init__()
        self.name = name
        self.expected_digest = expected_digest
        self._parent = None
        self.children = []
        self.metadata = metadata or {}

    children: typing.Iterable[transforms.Transform]

    @property
    def parent(self) -> "transforms.Transform":
        return self._parent

    @parent.setter
    def parent(self, value: "transforms.Transform") -> None:
        if self._parent is not None:
            raise RuntimeError(f"{self} is already associated with a transform")
        self._parent = value

    @property
    def digest(self) -> str:
        return None

    @property
    def is_stale(self) -> bool:
        if self._parent:
            return self in self._parent.stale_outputs
        return False

    def __repr__(self):
        return f"{self.__class__.__name__}({self.name})"

    async def execute(self):
        if self.parent:
            await self.parent
        if self.expected_digest and self.digest != self.expected_digest:
            # Pop the composite digest to ensure this artifact is regenerated.
            self.metadata.pop("last_composite_digest", None)
            raise ValueError(f"expected digest `{self.expected_digest}` but got "
                             f"`{self.digest}` for `{self}`")


class Group(Artifact):
    """
    Artifact representing a group of other artifacts.

    Args:
        name: Name of the group. The group name is added as a prefix for all artifacts created
            within the context manager of the group.
        metadata: Metadata which may persist across runs.

    Attributes:
        members: Members of the group.
    """
    def __init__(self, name: str, metadata: dict = None) -> None:
        super().__init__(name, expected_digest=None, metadata=metadata)
        self.members = []

    def __enter__(self) -> "Group":
        if self in self.STACK:  # pragma: no cover
            raise RuntimeError(f"{self} is already in the stack")
        self.STACK.append(self)
        return self

    def __exit__(self, *_):
        if (other := self.STACK.pop()) is not self:  # pragma: no cover
            raise RuntimeError(f"expected the last element to be {self} but got {other}")

    async def execute(self):
        await asyncio.gather(*self.members)

    @property
    def is_stale(self) -> bool:
        return any(member.is_stale for member in self.members)

    STACK: list["Group"] = []

    @classmethod
    def evaluate_qualified_name(cls, name: str) -> str:
        """
        Evaluate the fully qualified name of an artifact given its context.

        Args:
            name: Unqualified name.

        Returns:
            name: Qualified name.
        """
        if not cls.STACK:
            return name
        return os.path.join(cls.STACK[-1].name, name)

    @classmethod
    def append(cls, artifact: Artifact) -> None:
        """
        Append an artifact to the current group if available.

        Args:
            artifact: Artifact to append to the current group.
        """
        if not cls.STACK:
            return
        cls.STACK[-1].members.append(artifact)


@contextlib.contextmanager
def group_artifacts(*names: str, squeeze=True) -> list[Group]:
    """
    Group artifacts in a context.

    Args:
        names: Name of nested groups.
        squeeze: Whether to return a single element if there is only one group.

    Returns:
        groups: Artifacts representing the nested groups.
    """
    groups = []
    try:
        for name in names:
            groups.append(Group(name).__enter__())
        yield groups[0] if len(groups) == 1 and squeeze else groups
    finally:
        [group.__exit__() for group in reversed(groups)]


class File(Artifact):
    """
    Artifact representing a file.

    Args:
        name: Unique name of the artifact.
        expected_digest: Digest expected when the artifact is available.
        metadata: Metadata which may persist across runs.
    """
    def __init__(self, name: str, expected_digest: str = None, metadata: dict = None) -> None:
        super().__init__(name, expected_digest, metadata)
        if re.search(r"\s", self.name):
            LOGGER.warning("whitespace in `%s` is a recipe for disaster; expect the unexpected",
                           self.name)

    async def execute(self):
        # Omit directory creation and existence checks during dry run.
        if transforms.Transform.DRY_RUN:
            await super().execute()
            return

        # Create the parent directory if necessary.
        if dirname := os.path.dirname(self.name):
            os.makedirs(dirname, exist_ok=True)

        # Generate the artifact and verify the file exists.
        await super().execute()
        if not os.path.exists(self.name):
            if self._parent:
                message = f"{self._parent} did not generate {self}"
            else:
                message = f"{self} does not exist and cannot be generated"
            raise FileNotFoundError(message)

    @property
    def digest(self):
        try:
            # Return the digest if the file hasn't been modified since we last computed the digest.
            last_modified = os.stat(self.name).st_mtime
            cache_modified = self.metadata.get("last_modified")
            cache_digest = self.metadata.get("last_digest")
            if cache_digest and cache_modified and cache_modified >= last_modified:
                LOGGER.debug("returned cached digest for `%s`", self)
                return cache_digest

            # Evaluate the digest.
            algorithm = util.Crc32()
            with open(self.name, "rb") as fp:
                while (chunk := fp.read(4096)):
                    algorithm.update(chunk)
            digest = algorithm.hexdigest()
            self.metadata["last_modified"] = last_modified
            self.metadata["last_digest"] = digest
            LOGGER.debug("evaluated digest for `%s`", self)
            return digest
        except FileNotFoundError:
            return None

    def read(self) -> str:
        """
        Read the file contents.
        """
        with open(self.name) as fp:
            return fp.read()

    @classmethod
    def glob(self, pattern: str) -> typing.Iterable["File"]:
        """
        Create a list of file artifacts based on a glob pattern.

        Args:
            pattern: Pattern passed to :func:`glob.glob`.

        Returns:
            artifacts: File artifacts.
        """
        return normalize_artifacts([filename for filename in glob.glob(pattern)])

    def __str__(self):
        return shlex.quote(self.name)


def normalize_artifacts(
    artifacts: typing.Union[str, Artifact, typing.Iterable[typing.Union[str, Artifact]]]) \
        -> typing.Iterable[Artifact]:
    r"""
    Normalize one or more artifacts. Strings are implicitly converted to :class:`File`\ s.

    Args:
        artifacts: One or more artifacts or transforms.

    Returns:
        normalized: Normalized artifacts.
    """
    if artifacts is None:
        return []
    if not isinstance(artifacts, typing.Iterable) or isinstance(artifacts, str):
        artifacts = [artifacts]
    normalized = []
    for artifact in artifacts:
        if isinstance(artifact, (str, pathlib.Path)):
            normalized.append(File(artifact))
        elif isinstance(artifact, Artifact):
            normalized.append(artifact)
        elif isinstance(artifact, transforms.Transform):
            normalized.extend(artifact.outputs)
        else:
            raise TypeError(artifact)
    return normalized


async def gather_artifacts(*artifacts_or_transforms, num_concurrent: int = None) \
       -> typing.Coroutine:
    """
    Gather one or more artifacts and wrap them in a coroutine.

    Args:
        *artifacts_or_transforms: Artifacts to gather and/or transforms whose artifacts to
            gather.
        num_concurrent: Maximum number of concurrent transforms. Defaults to unrestricted.

    Returns:
        gathered: Coroutine that awaits all gathered artifacts.
    """
    gathered = []
    for item in artifacts_or_transforms:
        if isinstance(item, Artifact):
            gathered.append(item)
        elif isinstance(item, transforms.Transform):
            gathered.extend(item)
        else:
            raise TypeError(item)
    with transforms.Transform.limit_concurrency(num_concurrent):
        await asyncio.gather(*gathered)
